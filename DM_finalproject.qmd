---
title: "DM_finalproject"
author: Paula and Esther
format: 
  html:
    toc: true
    number-sections: true
execute:
  echo: true
  warning: false
  message: false
---

## Introduction

The link to our github repository : <https://github.com/estal8/Data-management---final-project.git>

## Research question

Our project studies the socio-economic and demographic determinants of crime in the United States at the state level. More precisely, we ask whether economic opportunities and social inequalities are correlated with different types of crime (violent and property crime) across states and over time.

Crime in the United States exhibits substantial variation across states and over time, and understanding the socio-economic factors that contribute to these differences remains a central question in both economics and public policy. Our project aims to investigate how economic conditions, demographic structure, inequality, public spending, and police behaviour correlate with violent and property crime at the state–year level. The objective is not to establish strict causal relationships—an ambitious task given the observational nature of the data—but rather to document robust associations and explore whether crime levels systematically co-evolve with structural socio-economic indicators.

A first motivation for this research question is the long-standing hypothesis that economic opportunity and crime are linked. Periods of slow economic growth and labour market stress may increase incentives for certain types of offences, particularly property crime. To examine this mechanism, we combine the crime data from the CORGIS/FBI series with annual state GDP from the Bureau of Economic Analysis. GDP provides a measure of overall economic activity and allows us to test whether states experiencing strong or weak economic performance also display different crime patterns.

Income inequality is another potential determinant of criminal activity. The World Inequality Database (WID) provides rich information on income shares and fiscal aggregates at the top and bottom of the distribution. High inequality may generate both economic stress and social fragmentation, potentially amplifying incentives for crime or reducing the perceived legitimacy of institutions. By merging the WID data with the crime series, we can evaluate whether states with higher income concentration or lower fiscal income shares among the majority of the population exhibit systematically different violent or property crime rates.

Migration flows constitute a third dimension of interest. Public debate often assumes—sometimes incorrectly—that immigration influences crime rates. The DHS immigration data enable us to incorporate yearly state-level measures of lawful permanent residents, nonimmigrants, asylees, and refugees. Our goal is not to test politically charged claims, but to empirically examine whether changes in migration patterns are associated with changes in state crime trends once we control for other socio-economic conditions.

In addition, we incorporate information on public expenditure and food assistance (from the Census and USDA Food Environment Atlas), which may proxy for the strength of social safety nets and local investment in welfare-related programmes. States with stronger social services may provide better support for vulnerable populations, potentially mitigating some of the socio-economic pressures that contribute to crime. Including these variables allows us to explore such channels empirically.

Finally, we integrate data on police shootings from the Washington Post database. While this dataset does not measure police activity directly, fatal shootings can serve as a proxy for law-enforcement intensity or tension between police and residents. Investigating whether states with higher levels of fatal shootings also differ in crime trends contributes to a broader understanding of institutional and behavioural dimensions of crime.

How do economic conditions, income inequality, immigration flows, public spending, and police activity correlate with violent and property crime across U.S. states?

This integrated approach provides a comprehensive framework for analysing the socio-economic determinants of crime in the contemporary United States.

## Description of our data

The **crime per state (1960-2019) database** from the **CORGIS Dataset Project** (Collection of Really Great, Interesting, Situated Datasets). This dataset was produced by the U.S. Department of Justice and the Federal Bureau of Investigation. The crimes described in this dataset are divided into two categories : violent crimes and property crimes (much more numerous), which encompass themselves different types of crimes : 

- violent crimes : assaults, murders, rapes, and robberies.

- property crimes : burglaries, larcenies, and motor crimes.

The database combines rates per 100 000 population and total number of crimes per states, which we will mainly focus on. We also obtain data on the total state population for each year.

<https://corgis-edu.github.io/corgis/csv/state_crime/>


The **World Inequality Database** (WID), for every US state. The WID combines national accounts with survey data and tax sources, enabling the publication of more reliable estimates of inequality covering the entire distribution of income and wealth. It was initially created in January 2011 under the name World Top Incomes Database (WTID) and offers the most comprehensive collection of historical series on wealth inequality available to date. We only downloaded data for each US state.

<https://wid.world/fr/donnees/>


The **police shootings database (2015-2024)** we used is from the Washington Post. In 2015, the Washington Post started compiling data on murders caused by police shootings. The data gathered is much larger than what we need (threat type, flee status, name and age of the victime, etc). We will calculate the number of people who have died because of a police shooting by state.

<https://github.com/washingtonpost/data-police-shootings/tree/master> 


The **immigration per state database (2013-2023)** is from the Office of Homeland Security of the United States. The tables provide totals for lawful permanent residents (LPRs), new arrivals and adjustments; nonimmigrant arrivals; naturalizations; refugee arrivals; and total asylum grants.

<https://ohss.dhs.gov/topics/immigration/state-immigration-data>


The **total per capita categorized public spending** (education, welfare, health, corrections, etc.) **for every US state (2017-2023)** database is from the United States Census Bureau.

<https://data.census.gov/table/GOVSTIMESERIES.GS00LF01> 


The **Food Environment Atlas (2012-2023)** from the **US Department for Agriculture** (USDA). It incorporates data from the Food Access Research Atlas, the Household Food Security in the United States, SNAP Policy Database, Atlas of Rural and Small-Town America, and Poverty Area Measures. 

<https://www.ers.usda.gov/data-products/food-environment-atlas/data-access-and-documentation-downloads>


**Regional Economic Accounts** from the **Bureau of Economic Analysis** of the US. We only downloaded the SASUMMARY table, that compiles State Annual Summary Statistics (Personal Income, GDP, Consumer Spending, Price Indexes, and Employment).

<https://apps.bea.gov/regional/downloadzip.htm>

**Local Area Unemployment Statistics (LAUS)** from the **US Bureau of Labor Statistics (BLS)**. It spans from 1976 to 2025. We directly downloaded it from the BLS website in the excel format and then transformed it into a csv file.

<https://www.bls.gov/web/laus/ststdsadata.zip>

The **poverty database** from the **United States Census Bureau**. It compiles data on the poverty rate and the poverty population per state, which we will use. <https://www.census.gov/data/tables/time-series/demo/income-poverty/historical-poverty-people.html>

The **Prison population over time (1980-2022)** database from **The Sentencing Database** compiles data on the carceral population for each US state. <https://www.sentencingproject.org/research/> 

Data from the **Apparent Per Capita Alcohol Consumption: National, State, and Regional Trends, 1977–2022**, produced by the **National Institute on Alcohol Abuse and Alcoholism**. The findings are drawn from alcoholic beverage sales data, collected by the Alcohol Epidemiologic Data System. We used Python to download the txt file and transform it into a csv.

### Data cleaning 

We first import each raw CSV file and check for missing values and types.  
For the crime dataset, we simply confirm that there are no missing values and keep the state–year level information on violent and property crime rates and total population.

For the WID data, we download one CSV file per state, bind them into a single long table, and restrict attention to a small set of income and inequality variables. We then pivot the data to a wide state–year format (`wid_wide`) so that each row corresponds to a state–year observation.

For the police shootings data, we recode state abbreviations into full state names, extract the year of each incident, and aggregate to the state–year level, computing the total number of shootings, and the number of unarmed victims, gun-related incidents, and cases with body cameras.

For the immigration data, we keep a subset of relevant variables (total population, lawful permanent residents, nonimmigrants, refugees, asylees, and their per-million measures) and keep only state–year combinations with non-missing values.

For the Food Environment Atlas, which is provided at the state level only, we parse all numeric variables and retain three indicators related to WIC, school lunch and school breakfast participation in 2015.

For the BEA state income data, we filter the SASUMMARY table to keep only the line corresponding to Gross Domestic Product (GDP), convert year columns to long format (one row per state–year), and parse GDP values as numeric.

Finally, we perform a sequence of left joins on the common keys (`State`, `Year`) to build a panel dataset combining crime, inequality, police shootings, immigration, GDP and food assistance indicators.

```{r}
#| message: false
#| echo: false

library(dplyr)
library(here)
library(vroom)
library(ggplot2)
library(tidyr)
library(stringr)
library(readr)
```

```{r}
#| message: false

here::i_am("Data-management---final-project.Rproj")
```

### Loading and cleaning each database

#### The crime database

```{r}
#| message: false

crime <- vroom(here("DATA_statecrime.csv"))
```

```{r}
crime |>
  summarise(`Number of crimes` = n(), `Number of variables` = ncol(crime)) |>
  knitr::kable()
```

```{r}
crime |>
  summarise(across(everything(), ~ sum(is.na(.)))) |>
  pivot_longer(everything(), names_to = "variable", values_to = "nb_na") |>
  knitr::kable()
```
There are no missing values.


#### The WID database

First, we must join all 51 states bases of the WID. To do so, we are going to create a list with all the files' names to avoid having to manually download all of them.

```{r}
codes_states <- c(paste0("US-", state.abb), "US-DC")
```

Here, we recreate the 51 file names.

```{r}
file_names <- here("DATA_wid", paste0("WID_data_", codes_states, ".csv"))
```

Since the files don't have the full state names, we need to turn the abbreviations into the actual states names. To do so, we create a data base that associates each abbreviation to the state.

```{r}
# Using R's built-in state.abb and state.name vectors
dict_states <- tibble(
  country = paste0("US-", state.abb),
  State   = state.name
) |>
  bind_rows(tibble(country = "US-DC", State = "District of Columbia"))
```

Now, we create a list with all the file names and download all files.

```{r}
wid_all <- vroom(
  file = file_names,
  delim = ";",
  show_col_types = FALSE
)
```

We check that we have the right number of states.

```{r}
nrow(wid_all)
n_distinct(wid_all$country)
```

We have 51 states because the District of Columbia is counted as one here. The variable is named country because the WID is originally for countries and not for regions within countries.

Now, checking for missing values : 

```{r}
wid_all |>
 summarise(
   na_country = sum(is.na(country)),
   na_variable = sum(is.na(variable)),
   na_percentile = sum(is.na(percentile)),
   na_year = sum(is.na(year)),
   na_value = sum(is.na(value)),
   na_age = sum(is.na(age)),
   na_pop = sum(is.na(pop))
 ) |>
  knitr::kable()
```

There are no missing values.

As for the types : 

```{r}
sapply(wid_all, typeof)
```

We also want to clarify what the variable codes stand for. We use the variable list that was provided when downloading the WID data, and then join the newly created database to our **wid_all** base.

```{r}
dict_variables <- tibble(
 variable = c("afiinct992", "mfiinct992", "npopult992",
              "ntaxret992", "sfiinct992", "sptinct992"),
 new_name = c("avg_fiscal_income", "total_fiscal_income", "population",
               "nb_tax_returns", "share_fiscal_income", "share_pretax_income")
)
```

```{r}
wid_all <- wid_all |>
  left_join(dict_variables, by = "variable") |>
  select(!variable) |>
  left_join(dict_states, by = "country") |>
  select(!country) |>
  relocate(State, new_name)
```

The database now looks like this : 

```{r}
wid_all |>
 slice_head(n = 10) |>
  knitr::kable()
```

We need to pivot it. But before doing so we can clarify the variables by sorting them for each percentile : 

```{r}
wid_all <- wid_all |>
  mutate(var_complete = str_c(new_name, percentile, sep = "_"))
```

Now we can pivot :

```{r}
wid_wide <- wid_all |>
  select(State, year, var_complete, value) |>
  pivot_wider(
    id_cols = c(State, year),
    names_from = var_complete,
    values_from = value
  ) |>
  rename(Year = year) # else merging does not work
```

The table that summarises this database : 

```{r}
wid_wide |>
  summarise(
    nb_lines = n(),
    nb_states = n_distinct(State),
    nb_years = n_distinct(Year)
  ) |>
  knitr::kable()
```

We should check again for missing values.

```{r}
wid_wide |>
  summarise(across(everything(), \(x) sum(is.na(x)))) |>
  pivot_longer(everything(), names_to = "variable", values_to = "nb_na") |>
  filter(nb_na > 0) |>
  slice_head(n=5) |> # used because too many missing values
  knitr::kable()
```

Looking at the variables doesn't allow us to locate the missing values, so maybe we should try by state :

```{r}
wid_wide |>
  summarise(nb_na = sum(is.na(across(everything()))), .by = State) |>
  filter(nb_na > 0) |>
  knitr::kable()
```

The huge number of missing values for Alaska can probably be explained by the fact it only became a US state in 1959. Since the crime database begins in 1960, these won't be considered in the analysis anyways. As for the rest of the NAs, considering the huge amount of values (more than 230 000), they are negligeable.

#### Police Shooting database
```{r}
#| message: false

police_shootings <- vroom(here("DATA_policeshootingto2024.csv"))
sapply(police_shootings, typeof)
```

```{r}
dict_states_abbrev <- tibble(
  state = state.abb,
  State = state.name
) |>
  bind_rows(tibble(state = "DC", State = "District of Columbia"))
```

```{r}
police_shootings |>
  summarise(across(everything(), ~ sum(is.na(.)))) |>
  pivot_longer(everything(), names_to = "variable", values_to = "nb_na") |>
  filter(nb_na > 0) |>
  knitr::kable()
```

We only need the year from the date : 
```{r}
police_shootings <- police_shootings |>
  left_join(dict_states_abbrev, by = "state") |>
  mutate(
    Year = as.integer(substr(as.character(date), 1, 4))
  ) |>
  select(-state)   
```

Now, we calculate the number of crimes per state and per year. We also keep the data on body cameras, and the armed or unarmed status of the people who were killed by the police.

```{r}
police_shootings_final <- police_shootings |>
  summarise(
    nb_police_shootings = n(),
    nb_unarmed          = sum(armed_with == "unarmed", na.rm = TRUE),
    nb_gun              = sum(armed_with == "gun",     na.rm = TRUE),
    nb_body_camera      = sum(body_camera,            na.rm = TRUE),
    .by = c(State, Year)
  )
```

#### Immigration database 

```{r}
#| message: false

immigration <- vroom(here("DATA_stateimmigration.csv"))
```

```{r}
sapply(immigration, typeof)
```

We look at the missing values per variables to better locate them. Since there are a lot of variables (31), we won't list them as before.

```{r}
immigration |>
  summarise(across(everything(), \(x) sum(is.na(x)))) |>
  pivot_longer(everything(), names_to = "variable", values_to = "nb_na") |>
  filter(nb_na > 0) |>
  knitr::kable()
```

These missing values are properly encoded and are associated to a total of refugees equal to 0.

```{r}
immigration |>
  summarise(
    `Number of rows`    = n(),
    `Number of columns` = ncol(immigration)
  ) |>
  knitr::kable()
```

```{r}
immigration_small <- immigration |>
  transmute(
    State,
    Year,
    imm_population = Population,
    lpr_total      = `Lawful Permanent Residents Total`,
    nonimm_total   = `Nonimmigrants Total`,
    refugees_total = `Refugees Total`
  )
```

We construct a reduced immigration dataset, keeping only a few key indicators (total population, lawful permanent residents, nonimmigrants and refugees) at the state–year level.

#### Total per capita categorized public spending

```{r}
#| message: false

public_spending <- vroom(here("DATA_publicspending", "DATA_publicspending20172023.csv"))
```

```{r}
public_spending |>
  summarise(
    `Number of rows`    = n(),
    `Number of columns` = ncol(public_spending)
  ) |>
  knitr::kable()
```

```{r}
public_spending |> slice_head(n = 10) |> knitr::kable()
```

```{r}
# Removing the US category and selecting only the categories of interest within the public spending 
public_spending_clean <- public_spending |>
  filter(NAME != "United States") |>
  filter(GOVTYPE == "001") |>
  filter(
    AGG_DESC_LABEL == "Expenditure - Direct Expenditure - General Expenditure - Education - Total Expenditure" |
    AGG_DESC_LABEL == "Expenditure - Direct Expenditure - General Expenditure - Public Welfare - Total Expenditure" |
    AGG_DESC_LABEL == "Expenditure - Direct Expenditure - General Expenditure - Health - Total Expenditure" |
    AGG_DESC_LABEL == "Expenditure - Direct Expenditure - General Expenditure - Correction - Total Expenditure" |
    AGG_DESC_LABEL == "Expenditure - Direct Expenditure - General Expenditure - Police Protection - Total Expenditure"
  ) 
```

```{r}
# Clarifying the variables and sorting them by spending category
public_spending_clean <- public_spending_clean |>
  mutate(
    State = NAME,
    Year = as.integer(time),
    category = case_when(
      str_detect(AGG_DESC_LABEL, "Education") ~ "spending_education", 
      str_detect(AGG_DESC_LABEL, "Welfare") ~ "spending_welfare",
      str_detect(AGG_DESC_LABEL, "Health") ~ "spending_health",
      str_detect(AGG_DESC_LABEL, "Correction") ~ "spending_corrections",
      str_detect(AGG_DESC_LABEL, "Police") ~ "spending_police"
    ),
    amount = readr::parse_number(AMOUNT)
    ) 
```

```{r}
# pivoting
public_spending_final <- public_spending_clean |>
  select(State, Year, category, amount) |>
  pivot_wider(
    id_cols = c(State, Year),
    names_from = category,
    values_from = amount
  )
```

```{r}
sapply(public_spending, typeof)
```

#### Food Environment Atlas

```{r}
#| message: false

food_data <- vroom(here("FOOD ACCESS", "DATA_foodenvi.csv"))
food_variables <- vroom(here("FOOD ACCESS", "DATA_foodenvi-variables.csv")) # the names of the variables, needed to clarify the variables in the main database
```

```{r}
food_data_num <- food_data |>
  drop_na() |> # the NA are full lines
  dplyr::mutate(
    dplyr::across(
      -c(StateFIPS, State),  
      ~ readr::parse_number(.x)
    )
  )
```

We convert all quantitative columns stored as character strings into numeric values using parse_number, leaving only the state identifiers as text.

```{r}
food_state <- food_data_num |>
  dplyr::transmute(
    State,
    wic_2015   = `WIC participants, FY 2015`,
    nslp_2015  = `National School Lunch Program participants, FY 2015`,
    sbp_2015   = `School Breakfast Program participants, FY 2015`
  )

food_state |>
  summarise(nb_states = n_distinct(State)) |>
  knitr::kable()
```
We extract three indicators of participation in food assistance and school meal programmes in 2015, and check that all 50 states plus DC are present.

#### State GDP and Consumption spending 

```{r}
#| message: false

state_income <- vroom(here("DATA_stateincome.csv"))

state_income |>
  summarise(
    `Number of rows`    = n(),
    `Number of columns` = ncol(state_income)
  ) |>
  knitr::kable()
```
We select only the rows corresponding to state-level GDP and consumption expenditures per capita per state, reshape the data from wide to long format (one row per state–year), parse GDP as numeric, and summarise the coverage in terms of years and states.

```{r}
state_gdp <- state_income |>

  dplyr::filter(stringr::str_detect(Description, "Gross domestic product")) |>

  dplyr::mutate(
    dplyr::across(
      dplyr::matches("^[0-9]{4}$"),
      as.character
    )
  ) |>

  tidyr::pivot_longer(
    cols      = dplyr::matches("^[0-9]{4}$"),
    names_to  = "Year",
    values_to = "gdp"
  ) |>
  dplyr::mutate(
    Year = as.integer(Year),
  
    gdp  = readr::parse_number(gdp),
    State = GeoName
  ) |>
  dplyr::select(State, Year, gdp) |>
  dplyr::filter(!is.na(Year), !is.na(gdp))

state_gdp |>
  summarise(
    nb_rows   = n(),
    nb_states = n_distinct(State),
    min_year  = min(Year),
    max_year  = max(Year),
    nb_na_gdp = sum(is.na(gdp)),
  ) |>
  knitr::kable()
```
This dataset contains many economic indicators in wide format, so we first filter the rows corresponding to “Gross domestic product”. The year columns in the original file have mixed data types (some numeric, some character), which prevents reshaping the data. To fix this, we convert all four-digit year columns to character before reshaping the dataset from wide to long format.

We then create a clean State–Year–gdp table by parsing GDP values as numeric and keeping only non-missing observations. This prepares the GDP data to be merged consistently with the other state-year datasets in our panel.

We do the same for per capita consumption : 

```{r}
state_consumption <- state_income |>

  dplyr::filter(stringr::str_detect(Description, "Per capita personal consumption expenditures")) |>

  dplyr::mutate(
    dplyr::across(
      dplyr::matches("^[0-9]{4}$"),
      as.character
    )
  ) |>

  tidyr::pivot_longer(
    cols      = dplyr::matches("^[0-9]{4}$"),
    names_to  = "Year",
    values_to = "Consumption"
  ) |>
  dplyr::mutate(
    Year = as.integer(Year),
  
    gdp  = readr::parse_number(Consumption),
    State = GeoName
  ) |>
  dplyr::select(State, Year, Consumption) |>
  dplyr::filter(!is.na(Year), !is.na(Consumption))

state_consumption |>
  summarise(
    nb_rows   = n(),
    nb_states = n_distinct(State),
    min_year  = min(Year),
    max_year  = max(Year),
    nb_na_consumption = sum(is.na(Consumption)),
  ) |>
  knitr::kable()
```

#### Unemployment 

```{r}
#| message: false 
state_unemployment_raw <- vroom(here("DATA_unemployment_BLS.csv"), na = c("-", "–", "NA"))
```

```{r}
state_unemployment <- state_unemployment_raw |>
  slice(-(1:7)) # to remove title lines
```

```{r}
colnames(state_unemployment) <- c(
  "fips",
  "state",
  "year",
  "month",
  "civilian_pop",
  "labor_force",
  "labor_force_pct",
  "employment",
  "employment_pct",
  "unemployment_level",
  "unemployment_rate"
)
```

```{r}
#| message: false

state_unemployment <- state_unemployment |>
  mutate(
    State = state,
    Year = as.character(year),
    Year = as.integer(Year),
    month = as.integer(month),
    unemployment_rate = as.numeric(unemployment_rate) / 10 # the comma was removed
  )
```

```{r}
state_unemployment <- state_unemployment |>
  filter(nchar(fips) == 2) # removing the counties that have FIPS with 3 numbers
```

```{r}
state_annual_unemployment <- state_unemployment |>
  select(State, Year, unemployment_rate) |>
  group_by(State, Year) |>
  summarise(
    unemployment_rate = mean(unemployment_rate, na.rm = TRUE),
    .groups = "drop"
  )
```

```{r}
state_annual_unemployment |>
  summarise(
    nb_rows    = n(),
    nb_states  = n_distinct(State),
    min_year   = min(Year, na.rm = TRUE),
    max_year   = max(Year, na.rm = TRUE),
    nb_na = sum(is.na(unemployment_rate))
  ) |>
  knitr::kable()
```

#### Poverty

```{r}
#| message: false

state_poverty <- vroom(here("DATA_poverty.csv"))
```

```{r}
state_poverty <- state_poverty |>
  slice(-(1:3)) # to remove title lines
```

```{r}
new_names <- as.character(state_poverty[1, ])
colnames(state_poverty) <- new_names
state_poverty <- state_poverty[-1, ]
```

```{r}
state_poverty <- state_poverty |>
  select(State, `Number in poverty`, `Percent in poverty`) |>
  slice(-(1:51)) |> # making sure the first line begins with a year
  mutate(Year = ifelse(str_detect(State, "^\\d{4}"), str_extract(State, "^\\d{4}"), NA)) |>
  fill(Year) |>
  filter(!str_detect(State, "^\\d{4}") & !str_detect(State, "^State$")) |>
  distinct(State, Year, .keep_all = TRUE)
```

Since the population is expressed in thousands : 

```{r}
state_poverty <- state_poverty |>
  mutate(
    Year = as.integer(Year),
    nb_poor = str_replace_all(`Number in poverty`, "[^0-9.]", ""),
    nb_poor = as.numeric(nb_poor) * 1000,
    percent_poor = str_replace_all(`Percent in poverty`, " ", ""),
    percent_poor = str_replace_all(percent_poor, ",", "."),
    percent_poor = as.numeric(percent_poor)
    )
```

```{r}
state_poverty |>
  summarise(
    nb_rows    = n(),
    nb_states  = n_distinct(State),
    min_year   = min(Year, na.rm = TRUE),
    max_year   = max(Year, na.rm = TRUE),
    nb_na_nb_poor = sum(is.na(nb_poor)),
    nb_na_percent_poor = sum(is.na(percent_poor))
  ) |>
  knitr::kable()
```

```{r}
state_poverty |>
  filter(is.na(nb_poor)) |>
  knitr::kable()
```

The missing values seem to be linked to the last lines of the table.

```{r}
state_poverty <- state_poverty |>
  filter(!is.na(nb_poor)) 
```

#### Prison population

```{r}
#| message: false

state_prisonpop <- vroom(here("DATA_prisonpopulation.csv"),
                         col_types = cols(.default = "c")
                         )
```

```{r}
state_prisonpop <- state_prisonpop |>
  slice(-(1)) # to remove title lines
```

```{r}
new_names <- as.character(state_prisonpop[1, ]) 
colnames(state_prisonpop) <- new_names
state_prisonpop <- state_prisonpop[-1, ]
colnames(state_prisonpop)[1] <- "Year" # Column Year is named NA
```

```{r}
state_prisonpop_final <- state_prisonpop |>
  mutate(across(everything(), ~ na_if(., "-")),
         Year = as.integer(Year),
         across(-Year, ~ str_remove_all(., " ")),
         ) |> # The NA are encoded as "-"
  filter(Year >= 1998) 
```

```{r}
state_prisonpop_final <- state_prisonpop_final |>
  select(-`U.S. Total`, -Federal) |>
  pivot_longer(
    cols = -Year,
    names_to = "State",
    values_to = "prison_population"
  )
```

```{r}
state_prisonpop_final <- state_prisonpop_final |>
  mutate(prison_population = str_replace_all(prison_population, "[^0-9.-]", ""),
         prison_population = as.numeric(prison_population)
         )
```

```{r}
state_prisonpop_final |>
  summarise(
    nb_rows    = n(),
    nb_states  = n_distinct(State),
    min_year   = min(Year, na.rm = TRUE),
    max_year   = max(Year, na.rm = TRUE),
    nb_na_prison_population = sum(is.na(prison_population)),
  ) |>
  knitr::kable()
```

There are 25 missing values because DC is not included.

#### Alcohol per capita

The following data is provided as a fixed-width text file containing state-level information on gallons of ethanol consumed, population estimates, and per capita consumption for both the 14+ and 21+ age groups.

```{r}
#| message: false

state_alcohol_raw <- read_lines(here("DATA_alcohol.txt"))
```

```{r}
data_start <- which(str_detect(state_alcohol_raw, "^\\d{4}\\s+\\d+\\s+\\d"))[1]
data_lines <- state_alcohol_raw[data_start:length(state_alcohol_raw)]
data_lines <- data_lines[str_length(str_trim(data_lines)) > 0]
```

We parse the fixed-width file according to the informations provided in the txt file, by extracting year, state FIPS code, beverage type, and per capita ethanol consumption. 

```{r}
state_alcohol <- read_fwf(
  I(data_lines),
  fwf_cols(
    Year = c(1, 4),
    state_code = c(6, 7),
    beverage_type = c(9, 9),
    gallons_beverage = c(11, 20),
    gallons_ethanol = c(22, 30), 
    pop_14plus = c(32, 40),
    ethanol_per_capita_14_raw = c(43, 47), # divide per capita gallons by 10,000 to obtain correct value)**
    decile_14 = c(49, 50),
    pop_21plus = c(52, 60),
    ethanol_per_capita_21_raw = c(63, 67), # divide per capita gallons by 10,000 to obtain correct value
    decile_21 = c(69, 70)
  ),
  col_types = cols(.default = "c"),
  na = c("", ".", "NA")
)
```

```{r}
state_codes <- tibble(
  state_code = c("1", "2", "4", "5", "6", "8", "9", "10", "11", "12",
                 "13", "15", "16", "17", "18", "19", "20", "21", "22", "23",
                 "24", "25", "26", "27", "28", "29", "30", "31", "32", "33",
                 "34", "35", "36", "37", "38", "39", "40", "41", "42", "44",
                 "45", "46", "47", "48", "49", "50", "51", "53", "54", "55", "56"),
  State = c("Alabama", "Alaska", "Arizona", "Arkansas", "California", "Colorado",
            "Connecticut", "Delaware", "District of Columbia", "Florida", "Georgia",
            "Hawaii", "Idaho", "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky",
            "Louisiana", "Maine", "Maryland", "Massachusetts", "Michigan", "Minnesota",
            "Mississippi", "Missouri", "Montana", "Nebraska", "Nevada", "New Hampshire",
            "New Jersey", "New Mexico", "New York", "North Carolina", "North Dakota",
            "Ohio", "Oklahoma", "Oregon", "Pennsylvania", "Rhode Island", "South Carolina",
            "South Dakota", "Tennessee", "Texas", "Utah", "Vermont", "Virginia",
            "Washington", "West Virginia", "Wisconsin", "Wyoming")
)
```

We only keep observations corresponding to beer (beverage type 3) and exclude regional aggregates (Northeast, Midwest, South, West) and national totals.

```{r}
state_alcohol_clean <- state_alcohol |>
  filter(as.integer(state_code) < 60) |> # we only keep the states
  filter(beverage_type == "3") |> # all beveradges
  mutate(
    Year = as.integer(Year),
    state_code = str_trim(state_code),
    beer_per_capita_14plus = as.numeric(str_trim(ethanol_per_capita_14_raw)) / 10000,
    pop_14plus = as.numeric(str_trim(pop_14plus)),
    pop_21plus = as.numeric(str_trim(pop_21plus))
  ) |>
  left_join(state_codes, by = "state_code")
```

We chose to construct the variable `beer_per_capita_14plus` using the gallons of ethanol per capita for age 14 and older, to capture all beer consumption, not only the legal one.

```{r}
state_alcohol_clean <- state_alcohol_clean |>
  select(
    State,
    Year,
    beer_per_capita_14plus, 
    pop_14plus
  ) |>
  filter(!is.na(State))
```

```{r}
state_alcohol_clean |>
  summarise(
    nb_rows = n(),
    nb_states = n_distinct(State),
    nb_years = n_distinct(Year),
    min_year = min(Year),
    max_year = max(Year),
    nb_na = sum(is.na(beer_per_capita_14plus))
  ) |>
  knitr::kable()
```

### Merging

#### Crime and WID 

Now, we can actually merge the WID database for all states with the crime database.

```{r}
wid_crime <- left_join(crime, wid_wide, by = c("State", "Year"))

nrow(wid_crime)
ncol(wid_crime)
```

```{r}
wid_crime |>
  summarise(nb_na = sum(is.na(across(everything()))), .by = State) |>
  filter(nb_na > 0) |>
  arrange(desc(nb_na)) |>
  knitr::kable()
```

The large number of NA can be explained by the presence of the US in the database for crime but not in the WID. We will only use it to plot the evolution of both property and violent crimes at the country level, so these NAs won't have any impact on our work.

```{r}
wid_us <- wid_crime |> 
  filter(State == "United States") # use this for the graph !!!
```

```{r}
wid_crime <- wid_crime |>
  filter(State != "United States")
```

As for the rest, it is likely due to the fact that the WID goes to 2018 and the crime database to 2019. We can see it here by looking at the WID data :

```{r}
wid_crime |>
  filter(Year == 2019) |>
  slice_head(n = 5) |>
  select(26:last_col()) |> # I'm only printing these columns because before are the variables for crime, which don't have any NA.
  knitr::kable()
```

So, we can keep only the years both databases have in common :

```{r}
wid_crime <- wid_crime |>
  filter(Year <= 2018)
```

Now, we should have much less NAs :

```{r}
wid_crime |>
  summarise(nb_na = sum(is.na(across(everything()))), .by = State) |>
  arrange(desc(nb_na)) |>
  slice_head(n = 5) |> # because there are 0 missing values for all states
  knitr::kable()
```

#### Final panel construction

We sequentially merge the crime–WID panel with police shootings, immigration, consumption, public spendings and GDP, always using the common keys (State, Year). There will be a lot of missing values since the years the databases cover are different.

```{r}
panel_full <- wid_crime |>
  left_join(police_shootings_final, by = c("State", "Year")) |>
  left_join(immigration_small,      by = c("State", "Year")) |>
  left_join(state_gdp,               by = c("State", "Year")) |>
  left_join(state_consumption,       by = c("State", "Year")) |>
  left_join(public_spending_final,   by = c("State", "Year")) |>
  left_join(state_annual_unemployment, by = c("State", "Year")) |>
  left_join(state_prisonpop_final, by = c("State", "Year")) |>
  left_join(state_poverty, by = c("State", "Year")) |>
  left_join(state_alcohol_clean, by = c("State", "Year"))
```

```{r}
panel_full <- panel_full |>
  left_join(food_state, by = "State")
```

```{r}
panel_full |>
  summarise(
    `Number of rows`    = n(),
    `Number of columns` = ncol(panel_full),
    `Number of states`  = n_distinct(State),
    `Number of years`   = n_distinct(Year)
  ) |>
  knitr::kable()
```

### Selecting the variables of interest

#### Main variables

Our main *outcome* variables are:

- **Violent crime rate** (`Data.Rates.Violent.All`): number of violent crimes (assault, murder, rape, robbery) per 100,000 inhabitants at the state–year level.
- **Property crime rate** (`Data.Rates.Property.All`): number of property crimes (burglary, larceny, motor vehicle theft) per 100,000 inhabitants.

Key *explanatory* variables include:

- **Income inequality and income shares** from WID (e.g. `share_pretax_income_p90p100`), capturing the share of total income accruing to the top of the distribution.
- **Immigration stocks and flows** (e.g. `imm_population`, `lpr_total`, `nonimmigrants_total`, `refugees_total`) from DHS, measured at the state–year level.
- **State GDP** (`gdp`) and personal income from the BEA, describing the size and dynamics of state economies.
- **Per capita personal consumption expenditures** also from the BEA (`Consumption`).
- **Public spending indicators** from the Census (education, welfare, health, corrections), describing the composition of state and local public expenditure.
- **Food assistance participation** (`wic_2015`, `nslp_2015`, `sbp_2015`) from the Food Environment Atlas, as proxies for food insecurity and social protection.
- **Police shootings** (`nb_police_shootings`, `nb_unarmed`, `nb_gun`, `nb_body_camera`), summarising the number and characteristics of fatal police shootings in each state–year.
- **Unemployment rates** per states (1976-2025) (`unemployment_rate`) from the BLS.
- **Prison population** per states (1980-2022) (`prison_population`).
- **Poverty rates and number** per states (1980-2023) (`nb_poor`, `Percent in poverty`).
- **Alcohol consumption** per states (1970-2022), more specifically beer consumption (`beer_per_capita_14plus`).

These variables are then merged into a panel dataset at the state–year level.

```{r}
panel_full_final <- panel_full |>
  filter(Year >= 1998) |>
  select(
    State,
    Year,

    # Crime rates
    Data.Rates.Violent.All,
    Data.Rates.Property.All,

    # Crime totals
    Data.Totals.Violent.All,
    Data.Totals.Property.All,

    # Population & income
    Data.Population,
    gdp,
    Consumption,
    share_pretax_income_p90p100,

    # Police shootings
    nb_police_shootings,
    nb_unarmed,
    nb_gun,
    nb_body_camera,

    # Labor market
    unemployment_rate,

    # Immigration
    imm_population,
    refugees_total,
    nonimm_total,
    lpr_total,

    # Public spending
    spending_corrections,
    spending_education,
    spending_welfare,
    spending_health,
    spending_police,

    # Food programs
    wic_2015,
    nslp_2015,
    sbp_2015,
    
    # Prison
    `prison_population`,
    
    # Poverty
    nb_poor, 
    percent_poor,
    
    # Alcohol
    beer_per_capita_14plus
  )
```

#### Graphical representation of the main target variables

```{r}
library(ggplot2)

crime_trends <- panel_full |>
  group_by(Year) |>
  summarise(
    violent_rate_mean  = mean(`Data.Rates.Violent.All`,  na.rm = TRUE),
    property_rate_mean = mean(`Data.Rates.Property.All`, na.rm = TRUE)
  ) |>
  pivot_longer(
    cols      = c(violent_rate_mean, property_rate_mean),
    names_to  = "type",
    values_to = "rate"
  )
```


```{r}
#| fig-cap: "Average violent and property crime rates in U.S. states"
#| fig-width: 8
#| fig-height: 4
#| warning: false
#| message: false

ggplot(crime_trends, aes(x = Year, y = rate, colour = type)) +
  geom_line(linewidth = 1) +
  labs(
    title  = "Average violent and property crime rates in U.S. states",
    x      = "Year",
    y      = "Crimes per 100,000 inhabitants",
    colour = "Crime type"
  ) +
  theme_minimal()
```

